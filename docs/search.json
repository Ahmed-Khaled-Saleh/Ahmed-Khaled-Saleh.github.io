[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ahmed Khaled",
    "section": "",
    "text": "Machine learning\n\n\nHOML\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nAhmed Khaled\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nMachine learning\n\n\nHOML\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nAhmed Khaled\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, My name is Ahmed Khaled, you can call me Ahmed. I am a machine learning engineer and a data scientist. This is my personal place on the internet where I create technical content and write about topics that I like and find interesting. I like to write because I first like it, and second, I want to see the gap in my knowledge when I write about something. My main area of interest lies in the intersection between computer science, mathematics, and artificial intelligence. I also like history, philosophy, psychology, and neuroscience."
  },
  {
    "objectID": "posts/homl-I/homl-part I.html",
    "href": "posts/homl-I/homl-part I.html",
    "title": "Hands-on Machine learning Chapter 1",
    "section": "",
    "text": "One of the most important books that was written in the field of machine learning ,that are easily accessible to a broad range of audience, is hands-on machine learning. Throughout a series of posts, I will try to explain a chapter by chapter. In this post, the first chapter is explained throughy.\nMachine learning applications are being deployed in all areas. From telecommunications to forecasting, all those applications help business decision making, and daily activities to make a better life. Facebook advertising is one example of such application that is so common to spot among the public. The following table is a summary of some applications:"
  },
  {
    "objectID": "posts/homl-I/homl-part I.html#what-is-machine-learning",
    "href": "posts/homl-I/homl-part I.html#what-is-machine-learning",
    "title": "Hands-on Machine learning Chapter 1",
    "section": "What is Machine learning",
    "text": "What is Machine learning\nMachine learning has a long history with different difinitions according to the era. The two most prevalent definitions of machine learning are:\n\nArthur Samuel\n\nField of study that gives computers the ability to learn without being explicitly programmed\n\nTom Mitchell \n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E\n\n\nThe two definitions are quite interesting, but the second one is a little bit more accurate."
  },
  {
    "objectID": "posts/homl-I/homl-part I.html#types-of-machine-learning",
    "href": "posts/homl-I/homl-part I.html#types-of-machine-learning",
    "title": "Hands-on Machine learning Chapter 1",
    "section": "Types of Machine learning",
    "text": "Types of Machine learning\nThe following diagram is a concptual representation of the different categories of machine learning algorithms:\n\n\nCategory 1: Method of supervision (Training type)\nIn this category, the idea is to classify the algorithms according to the way they are trained. The two main types are:\n\nSupervised learning\n\nThe algorithm is trained with labeled data. The algorithm learns from the data and tries to predict the label of new data. The two main types of supervised learning are classification and regression.\n\nUnsupervised learning\n\nThe algorithm is trained with unlabeled data. The algorithm tries to find patterns in the data. The two main types of unsupervised learning are clustering and dimensionality reduction.\n\n\n\n\nCategory 2: Method of adaptation\nThis is how the algorithm adapts to new data. The two main types are:\n\nBatch learning\n\nThe algorithm is trained with all the data available. The algorithm is then deployed to production. The algorithm is not trained again unless new data is available.\n\nOnline learning\n\nThe algorithm is trained with a small batch of data. The algorithm is then deployed to production. The algorithm is trained again with new data as it becomes available.\n\n\n\n\nCategory 3: Method of generalization (modeling type)\nThis is how the algorithm generalizes to new data. The two main types are:\n\nInstance-based learning\n\nThe algorithm is trained with labeled data. The algorithm learns from the data and tries to predict the label of new data. The two main types of instance-based learning are classification and regression.\n\nModel-based learning\n\nThe algorithm is trained with labeled data. The algorithm learns from the data and tries to predict the label of new data. The two main types of model-based learning are classification and regression."
  },
  {
    "objectID": "posts/homl-I/homl-part I.html#main-challenges-in-machine-learning",
    "href": "posts/homl-I/homl-part I.html#main-challenges-in-machine-learning",
    "title": "Hands-on Machine learning Chapter 1",
    "section": "Main challenges in Machine learning",
    "text": "Main challenges in Machine learning\nThe following diagram is a concptual representation of the main challenges in machine learning:"
  },
  {
    "objectID": "posts/homl-I/homl-part I.html#testing-and-validation",
    "href": "posts/homl-I/homl-part I.html#testing-and-validation",
    "title": "Hands-on Machine learning Chapter 1",
    "section": "Testing and validation",
    "text": "Testing and validation\nThe following diagram is a concptual representation of the main challenges in machine learning:"
  },
  {
    "objectID": "posts/homl-2/two.html",
    "href": "posts/homl-2/two.html",
    "title": "Chapter 2: End-to-End Machine Learning Project",
    "section": "",
    "text": "When starting a new code, try to import all the libraries you gonna need. The function init_session() is here to set the random seed to 42, so that the results are reproducible. It alse enables the warnings to be ignored (because this might be annoying)."
  },
  {
    "objectID": "posts/homl-2/two.html#exploring-the-data",
    "href": "posts/homl-2/two.html#exploring-the-data",
    "title": "Chapter 2: End-to-End Machine Learning Project",
    "section": "Exploring the data",
    "text": "Exploring the data\nNext stage is to gain some insights by looking at your data. This is one of the most important steps in the machine learning pipeline. The effect of this step is that it will unlock some structure in the data, like the outliers, the missing values, the noise, and the correlation between the features. This will help you in the next steps of the pipeline.\nWe shall start by visualizing the distribution of the features. This is done by plotting the histogram of each feature. The hist() method is used to plot the histogram. The bins argument is used to specify the number of bins. The figsize argument is used to specify the size of the figure. The plt.show() is used to display the figure.\n\nhousing.hist(bins=50, figsize=(10,8))\nplt.tight_layout()\nplt.show()\n\n\n\n\nFigure 1: A histogram of the numerical features.\n\n\n\n\nFigure 1 is a histogram of the numerical features. We will dive into the kind of visualization that we should use according to the data type and the task in a separate blog post. For now, we can say that for numerical features, hisograms are the most suitable visualization type to use."
  },
  {
    "objectID": "posts/homl-2/two.html#getting-the-data",
    "href": "posts/homl-2/two.html#getting-the-data",
    "title": "Chapter 2: End-to-End Machine Learning Project",
    "section": "Getting the data",
    "text": "Getting the data\nFirst things first, you should get the data from the source. This source could be a sensor reading, a database, a website, or a file. In this case, the data is a CSV file. You can download it from the author’s github repository. The function DataFetcher() is here to download the data from the source and return a pandas dataframe. The function takes the source as an argument. The source is the URL of the data. The function checks if the data is already downloaded, if not, it downloads it and returns the dataframe. The function also creates a datasets folder if it doesn’t exist.\n\ndef DataFetcher(*, source):\n\n    file_name = Path(source).name #housing.tgz\n    compressed_path = Path(os.path.join(\"datasets\", file_name))#datasets/housing.tgz\n    compressed_dir = str(compressed_path).split(\".\")[-2]#datasets/housing\n\n    if not compressed_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(source, compressed_path)\n        shutil.unpack_archive(compressed_path, os.path.join(\"datasets\"))\n    return pd.read_csv(os.path.join(compressed_dir, file_name.split(\".\")[-2] + \".csv\"))\n\n\nNote\n\nYou can notice the * being passed before the source as the first parameter. In fact this is not a parameter but it’s a feature in python >= 3.8 that makes the function take only keyword arguments. This is to make sure that the function is called with the source argument to avoid confusion when calling the function. In short, this enforces you to call all the function’s parameters after * using keyword arguments.\n\n\nNow you can call the function like this:\n\nurl = \"https://github.com/ageron/data/raw/main/housing.tgz\"\nhousing = DataFetcher(source= url)\n\nhousing is now a dataframe. You can check this using:\n\ntype(housing)\n\npandas.core.frame.DataFrame\n\n\nNow, you can take a look at the data using the head() method. This method returns the first 5 rows of the dataframe. You can also pass the number of rows you want to see as an argument.\n\nhousing.head()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n  \n    \n      0\n      -122.23\n      37.88\n      41.0\n      880.0\n      129.0\n      322.0\n      126.0\n      8.3252\n      452600.0\n      NEAR BAY\n    \n    \n      1\n      -122.22\n      37.86\n      21.0\n      7099.0\n      1106.0\n      2401.0\n      1138.0\n      8.3014\n      358500.0\n      NEAR BAY\n    \n    \n      2\n      -122.24\n      37.85\n      52.0\n      1467.0\n      190.0\n      496.0\n      177.0\n      7.2574\n      352100.0\n      NEAR BAY\n    \n    \n      3\n      -122.25\n      37.85\n      52.0\n      1274.0\n      235.0\n      558.0\n      219.0\n      5.6431\n      341300.0\n      NEAR BAY\n    \n    \n      4\n      -122.25\n      37.85\n      52.0\n      1627.0\n      280.0\n      565.0\n      259.0\n      3.8462\n      342200.0\n      NEAR BAY\n    \n  \n\n\n\n\nAnother handy function to summarize your data is the info() method. This method returns the number of rows, the number of columns, the column names, the data type of each column, and the number of non-null values in each column.\n\nhousing.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nYou can also see the Dtype column in the output of the info() method. This column shows the data type of each column. You can see that the ocean_proximity column is an object. This means that it’s a string. You can check the unique values of this column using the unique() method.\n\nhousing.ocean_proximity.unique()\n\narray(['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND'],\n      dtype=object)\n\n\nor you can use the value_counts() method to get the number of instances of each value.\n\nhousing.ocean_proximity.value_counts()\n\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n\n\nSince most columns (Also called features, dependent variable, predictors, attributes) are numerical, you can use the describe() method to get the summary of the numerical columns. This is quite useful to get a quick overview of the data. We can see that this is literally a sumaary of statistics. The count, mean, std, min, max, and the percentiles AKA descriptive statistics.\n\nhousing.describe()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n    \n  \n  \n    \n      count\n      20640.000000\n      20640.000000\n      20640.000000\n      20640.000000\n      20433.000000\n      20640.000000\n      20640.000000\n      20640.000000\n      20640.000000\n    \n    \n      mean\n      -119.569704\n      35.631861\n      28.639486\n      2635.763081\n      537.870553\n      1425.476744\n      499.539680\n      3.870671\n      206855.816909\n    \n    \n      std\n      2.003532\n      2.135952\n      12.585558\n      2181.615252\n      421.385070\n      1132.462122\n      382.329753\n      1.899822\n      115395.615874\n    \n    \n      min\n      -124.350000\n      32.540000\n      1.000000\n      2.000000\n      1.000000\n      3.000000\n      1.000000\n      0.499900\n      14999.000000\n    \n    \n      25%\n      -121.800000\n      33.930000\n      18.000000\n      1447.750000\n      296.000000\n      787.000000\n      280.000000\n      2.563400\n      119600.000000\n    \n    \n      50%\n      -118.490000\n      34.260000\n      29.000000\n      2127.000000\n      435.000000\n      1166.000000\n      409.000000\n      3.534800\n      179700.000000\n    \n    \n      75%\n      -118.010000\n      37.710000\n      37.000000\n      3148.000000\n      647.000000\n      1725.000000\n      605.000000\n      4.743250\n      264725.000000\n    \n    \n      max\n      -114.310000\n      41.950000\n      52.000000\n      39320.000000\n      6445.000000\n      35682.000000\n      6082.000000\n      15.000100\n      500001.000000\n    \n  \n\n\n\n\n\nCreating a Test Set OR Sampling 101\nAfter you get the data, you should keep a test set aside, this test set is used to evaluate the performance of your model. There are different ways to assess the quality of your test set. One such metric is the representativeness of it, which means that your test set should resemble the original data as much as possible. The test set should be kept aside until the end of the project to avoid the data snooping bias.\nSo, what is The data snooping bias? It is when you look at the test set and you tweak your model to perform better on the test set. This is not a good practice because you are not supposed to look at the test set until the end of the project.\nThere is a saying in statistics:\n    Torture the data until it confesses.\nThis is quite the same thing you ar doing here. You are tweaking the performance on a test set until it confesses. This is not a good practice. As a gneral rule, the test set is used to evaluate the performance of your model, not to tweak it. So, you should keep it aside until the end of the project.\nSo, the question is how to choose a test set. On can split the data selectively, for example, you can choose the last 20% of the data as the test set. This is not a good practice because:\n\nif you add new data, the last 20% of the data will change. So, you should not use this method. (your test result is not reproducible).\nIf your data is not evenly distributed, you might be trabbed in selective bias. For example, if you have a dataset of 1000 instances and 20% of them are from California, and for your case those 200 instances are the last 200 instances in your data. The result is you will have 200 instances from California in the test set and no other data from anywhere else. This is not a good practice because you will have a biased test set. (non-representativeness of the test set).\n\nThe last reason is an example of a phenomenon called Sampling Bias,one form of a bigger term Selection Bias, which refers to a falw in the process of choosing the data. Sampling Bias is a selection bias in the process of sampling a subset (sample) from the original data (the population).\nThere are many ways to sample a subset from the original data. The most common ones are:\n\nSimple Random Sampling\nStratified Random Sampling\nSystematic Random Sampling\nCluster Sampling\nMultistage Sampling\n\nThe first two are of interest for us in this chapter. So let us start by the simplest case, the simple random sampling.\n\nSimple Random Sampling\nThis process is quite simple. You just randomly select a subset of the data. This is the simplest way to sample a subset from the original data. You can do the following to sample a subset from the original data:\n\nShuffle the data based on the index.\n\nnp.random.permutation(len(data)) will return a list of indices of the data shuffled.\n\nDefine the size of the test set. You can specify a ration (0.2) and multiply it by the length of the data.\n\nint(len(data) * test_ratio) will return the size of the test set.\n\nGenerate two list of the indices; one for the training set and one for the test set.\n\ntest_indices = shuffled_indices[:test_set_size] will return the indices of the test set.\ntrain_indices = shuffled_indices[test_set_size:] will return the indices of the training set.\n\nUse the indices to select the data from the original data.\n\ndata.iloc[train_indices] will return the training set dataframe.\ndata.iloc[test_indices] will return the test set dataframe.\n\nThis works because train_indices is a list of integers, and the iloc method takes a list of integers as an argument and returns the corresponding rows.\n\nThe following code illustrates the process of simple random sampling:\n\nimport numpy as np\n\ndef shuffle_and_split_data(data, test_ratio):\n    np.random.seed(42)\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n\nThere are a problem with this approach:\n\nIf you run the program again, it will generate a different test set! Over time, you will get to see the whole dataset (data snooping bias again!!!), which is what we want to avoid.\n\nWe can mitigate this problem by one of the following solutions:\n\nSave the test set on the first run and then load it in subsequent runs.\n\nThe problem here is that, loading the test set every time is a waste of time and space. So, you should only do this if the dataset is quite small.\n\nSet the random number generator’s seed (e.g., np.random.seed(42) before calling np.random.permutation()) so that it always generates the same shuffled indices.\n\nThis could be tricky to understand why this solution is not perfect. So, let us try it:\n\n\n\ntrain_1, test_1 = shuffle_and_split_data(housing, 0.2)\n\n- Now, let us run the code again:\n\nhousing = DataFetcher(source=url)\ntrain_2, test_2 = shuffle_and_split_data(housing, 0.2)\n\nprint(train_1.equals(train_2))\ntest_1.equals(test_2)\n\nTrue\n\n\nTrue\n\n\n\nUse each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, you could compute a hash of each instance’s identifier, keep only the last byte of the hash, and put the instance in the test set if this value is lower or equal to 51 (256 * 0.2 = 51.2). This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.\n\n\n\nStratified Random Sampling\nThe following image illustrates the process of stratified random sampling:\n\n\n\nstratified random sampling"
  }
]