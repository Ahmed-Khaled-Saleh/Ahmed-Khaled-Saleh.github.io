{
  "hash": "b4fe26df924d79fcf470d7876bbca8e1",
  "result": {
    "markdown": "---\ntitle: 'Chapter 2: End-to-End Machine Learning Project'\nauthor: Ahmed Khaled\ndate: '2023-02-14'\ncategories:\n  - Machine learning\n  - HOML\ntoc: true\nimage: ../homl-I/img.png\nexecute:\n  freeze: auto\n  cache: true\n---\n\nWhen starting a new code, try to import all the libraries you gonna need. The function ```init_session()``` is here to set the random seed to 42, so that the results are reproducible. It alse enables the warnings to be ignored (because this might be annoying).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport random, os, sys\nimport shutil \nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport urllib\n\n\ndef init_session(RANDOM_SEED = 42):\n    RANDOM_SEED = RANDOM_SEED\n    np.random.seed(RANDOM_SEED)\n    random.seed(RANDOM_SEED)\n    import warnings\n    warnings.filterwarnings('ignore')\ninit_session()\n```\n:::\n\n\n## Getting the data\nFirst things first, you should get the data from the source. This source could be a sensor reading, a database, a website, or a file. In this case, the data is a CSV file. You can download it from the author's github repository. The function ```DataFetcher()``` is here to download the data from the source and return a pandas dataframe. The function takes the source as an argument. The source is the URL of the data. The function checks if the data is already downloaded, if not, it downloads it and returns the dataframe. The function also creates a datasets folder if it doesn't exist.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef DataFetcher(*, source):\n\n    file_name = Path(source).name #housing.tgz\n    compressed_path = Path(os.path.join(\"datasets\", file_name))#datasets/housing.tgz\n    compressed_dir = str(compressed_path).split(\".\")[-2]#datasets/housing\n\n    if not compressed_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        urllib.request.urlretrieve(source, compressed_path)\n        shutil.unpack_archive(compressed_path, os.path.join(\"datasets\"))\n    return pd.read_csv(os.path.join(compressed_dir, file_name.split(\".\")[-2] + \".csv\"))\n```\n:::\n\n\nNote\n: You can notice the * being passed before the source as the first parameter. In fact this is not a parameter but it's a feature in python >= 3.8 that makes the function take only keyword arguments. This is to make sure that the function is called with the source argument to avoid confusion when calling the function. In short, this enforces you to call all the function's parameters after * using keyword arguments.\n\nNow you can call the function like this:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nurl = \"https://github.com/ageron/data/raw/main/housing.tgz\"\nhousing = DataFetcher(source= url)\n```\n:::\n\n\nhousing is now a dataframe. You can check this using:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntype(housing)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\npandas.core.frame.DataFrame\n```\n:::\n:::\n\n\nNow, you can take a look at the data using the ```head()``` method. This method returns the first 5 rows of the dataframe. You can also pass the number of rows you want to see as an argument.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nhousing.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnother handy function to summarize your data is the ```info()``` method. This method returns the number of rows, the number of columns, the column names, the data type of each column, and the number of non-null values in each column.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nhousing.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n```\n:::\n:::\n\n\nYou can also see the Dtype column in the output of the ```info()``` method. This column shows the data type of each column. You can see that the ocean_proximity column is an object. This means that it's a string. You can check the unique values of this column using the ```unique()``` method.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nhousing.ocean_proximity.unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray(['NEAR BAY', '<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'ISLAND'],\n      dtype=object)\n```\n:::\n:::\n\n\nor you can use the ```value_counts()``` method to get the number of instances of each value.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nhousing.ocean_proximity.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: ocean_proximity, dtype: int64\n```\n:::\n:::\n\n\nSince most columns (Also called features, dependent variable, predictors, attributes) are numerical, you can use the ```describe()``` method to get the summary of the numerical columns. This is quite useful to get a quick overview of the data. We can see that this is literally a sumaary of statistics. The count, mean, std, min, max, and the percentiles AKA ```descriptive statistics```.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nhousing.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Creating a Test Set OR Sampling 101\n\nAfter you get the data, you should keep a test set aside, this test set is used to evaluate the performance of your model. There are different ways to assess the quality of your test set. One such metric is the **representativeness** of it, which means that your test set should resemble the original data as much as possible. The test set should be kept aside until the end of the project to avoid the **data snooping bias**.\n\nSo, what is The **data snooping bias**? It is when you look at the test set and you tweak your model to perform better on the test set. This is not a good practice because you are not supposed to look at the test set until the end of the project. \n\nThere is a saying in statistics:\n\n```\n    Torture the data until it confesses.\n```\n\nThis is quite the same thing you ar doing here. You are tweaking the performance on a test set until it confesses. This is not a good practice. As a gneral rule, the test set is used to evaluate the performance of your model, not to tweak it. So, you should keep it aside until the end of the project.\n\n\nSo, the question is how to choose a test set. On can split the data selectively, for example, you can choose the last 20% of the data as the test set. This is not a good practice because:\n\n- if you add new data, the last 20% of the data will change. So, you should not use this method. **(your test result is not reproducible)**.\n\n- If your data is not evenly distributed, you might be trabbed in selective bias. For example, if you have a dataset of 1000 instances and 20% of them are from California, and for your case those 200 instances are the last 200 instances in your data. The result is you will have 200 instances from California in the test set and no other data from anywhere else. This is not a good practice because you will have a biased test set. (**non-representativeness of the test set**).\n\nThe last reason is an example of a phenomenon called **Sampling Bias**,one form of a bigger term **Selection Bias**, which refers to a falw in the process of choosing the data. Sampling Bias is a selection bias in the process of sampling a subset (sample) from the original data (the population).\n\nThere are many ways to sample a subset from the original data. The most common ones are:\n\n- Simple Random Sampling\n- Stratified Random Sampling\n- Systematic Random Sampling\n- Cluster Sampling\n- Multistage Sampling\n\nThe first two are of interest for us in this chapter. So let us start by the simplest case, the simple random sampling.\n\n#### Simple Random Sampling\n\nThis process is quite simple. You just randomly select a subset of the data. This is the simplest way to sample a subset from the original data. You can do the following to sample a subset from the original data:\n\n\n- Shuffle the data based on the index.\n\n    - ```np.random.permutation(len(data))``` will return a list of indices of the data shuffled.\n\n- Define the size of the test set. You can specify a ration (0.2) and multiply it by the length of the data.\n\n    - ```int(len(data) * test_ratio)``` will return the size of the test set.\n\n\n- Generate two list of the indices; one for the training set and one for the test set.\n\n    - ```test_indices = shuffled_indices[:test_set_size]``` will return the indices of the test set.\n\n    - ```train_indices = shuffled_indices[test_set_size:]``` will return the indices of the training set.\n- Use the indices to select the data from the original data.\n    \n    - ```data.iloc[train_indices]``` will return the training set dataframe.\n\n    - ```data.iloc[test_indices]``` will return the test set dataframe.\n\n    This works because train_indices is a list of integers, and the ```iloc``` method takes a list of integers as an argument and returns the corresponding rows.\n    \nThe following code illustrates the process of simple random sampling:\n\n::: {#simple-random-sampling .cell alt='Simple Random Sampling' attr='align=\"center\"' caption='Simple Random Sampling' class='center' fig-class='center' height='400px' width='400px' execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\n\ndef shuffle_and_split_data(data, test_ratio):\n    np.random.seed(42)\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]\n```\n:::\n\n\nThere are a problem with this approach:\n\n- If you run the program again, it will generate a different test set! Over time, you will get to see the whole dataset ``(data snooping bias again!!!)``, which is what we want to avoid.\n\nWe can mitigate this problem by one of the following solutions:\n\n- Save the test set on the first run and then load it in subsequent runs.\n\n    - The problem here is that, loading the test set every time is a waste of time and space. So, you should only do this if the dataset is quite small.\n\n- Set the random number generator's seed (e.g., ```np.random.seed(42)``` before calling ```np.random.permutation()```) so that it always generates the same shuffled indices.\n\n    - This could be tricky to understand why this solution is not perfect. So, let us try it:\n\n::: {#random-seed-expirement .cell alt='Random seed expirement' attr='align=\"center\"' caption='Random seed expirement' class='center' fig-class='center' height='400px' width='400px' execution_count=11}\n``` {.python .cell-code}\ntrain_1, test_1 = shuffle_and_split_data(housing, 0.2)\n```\n:::\n\n\n    - Now, let us run the code again:\n\n::: {.cell alt='Random seed expirement 2' attr='align=\"center\"' caption='Random seed expirement 2' class='center' fig-class='center' height='400px' width='400px' execution_count=12}\n``` {.python .cell-code}\nhousing = DataFetcher(source=url)\ntrain_2, test_2 = shuffle_and_split_data(housing, 0.2)\n\nprint(train_1.equals(train_2))\ntest_1.equals(test_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue\n```\n:::\n\n::: {#random-seed-expirement-2 .cell-output .cell-output-display execution_count=12}\n```\nTrue\n```\n:::\n:::\n\n\n- Use each instance's identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, you could compute a hash of each instance's identifier, keep only the last byte of the hash, and put the instance in the test set if this value is lower or equal to 51 (256 * 0.2 = 51.2). This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.\n\n\n\n\n\n\n#### Stratified Random Sampling\n\n\n\nThe following image illustrates the process of stratified random sampling:\n\n![stratified random sampling](./stra.png)\n\n\n\n\n## Exploring the data\n\nNext stage is to gain some insights by looking at your data. This is one of the most important steps in the machine learning pipeline. The effect of this step is that it will unlock some structure in the data, like the outliers, the missing values, the noise, and the correlation between the features. This will help you in the next steps of the pipeline.\n\nWe shall start by visualizing the distribution of the features. This is done by plotting the histogram of each feature. The ```hist()``` method is used to plot the histogram. The ```bins``` argument is used to specify the number of bins. The ```figsize``` argument is used to specify the size of the figure. The ```plt.show()``` is used to display the figure.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nhousing.hist(bins=50, figsize=(10,8))\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A histogram of the numerical features.](two_files/figure-html/fig-hist-output-1.png){#fig-hist width=965 height=757}\n:::\n:::\n\n\n@fig-hist is a histogram of the numerical features. We will dive into the kind of visualization that we should use according to the data type and the task in a separate blog post. For now, we can say that for numerical features, hisograms are the most suitable visualization type to use.\n\n\n\n<!--\n\n\nhousing.columns\n\n\n\n\nhousing.ocean_proximity.value_counts() -->\n\n\n<!-- \nhousing.describe() \n -->\n\n\n<!-- housing.hist(bins=50, figsize=(20,15))\nplt.show()\n\n\n\n\n\ndef create_test_set(data, test_set_ratio= 0.2):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_set_ratio) # 20640 * 0.2 = 4128\n\n    test_set_indices = shuffled_indices[:test_set_size] # 0: 4127\n    train_set_indices = shuffled_indices[test_set_size:] # 4128:end\n\n    return data.iloc[train_set_indices], data.iloc[test_set_indices]\n\ntrain, test = create_test_set(housing)\n\n\n\nlen(train), len(test)\n\n\n\n\nfrom zlib import crc32\n\ndef is_id_in_test_set(identifier, test_ratio):\n    return crc32(np.int64(identifier)) < test_ratio * 2 ** 32\n\ndef split_data_with_id_hash(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = np.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n\n# housing_with_id = housing.reset_index() # adds an index column\n# train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\") \n\n\nfrom sklearn.model_selection import train_test_split\ntrain_set_random, test_set_random = train_test_split(housing, test_size= 0.2, random_state= 42)\nlen(train_set_random), len(test_set_random)\n\n\nhousing['income_cat'] = pd.cut(housing[\"median_income\"],\nbins= [0., 1.5, 3.0, 4.5, 6., np.inf], labels= [1, 2, 3, 4, 5])\n\n\nhousing['income_cat'].value_counts().sort_index().plot.bar(rot= 0, grid= True)\nplt.xlabel(\"Income Category\")\nplt.ylabel(\"Numbe of districts\")\nplt.show()\n\n\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplitter = StratifiedShuffleSplit(n_splits= 10, test_size= 0.2, random_state= 42)\n\nstrat_splits = []\n\nfor train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n    strat_train_set_n = housing.iloc[train_index]\n    strat_test_set_n = housing.loc[test_index]\n    strat_splits.append([strat_train_set_n, strat_test_set_n])\n\nstrat_train_set, strat_test_set = strat_splits[0]\n\n\n\nstrat_train_set, strat_test_set = train_test_split(housing, test_size= 0.2,\nstratify= housing[\"income_cat\"], random_state= 42)\n\n\n\nstrat_train_set[\"income_cat\"].value_counts() / len(strat_train_set)\n\n\n\nhousing = strat_train_set.copy()\n\n\n\nlen(housing)\n\n\n\nhousing.plot(kind= \"scatter\", x= \"longitude\", y=\"latitude\", grid= True,\ns= housing['population']/100, c= \"median_house_value\", cmap= \"jet\",\ncolorbar= True, legend= True, figsize= (10, 7), label= \"population\", sharex= False)\nplt.show()\n\n\n\ncorr_matrix = housing.corr()\ntype(corr_matrix)\n\n\n\ncorr_matrix[\"median_house_value\"].sort_values(ascending= False)\n\n\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nplt.show()\n --> \n\n",
    "supporting": [
      "two_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}